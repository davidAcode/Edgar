{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A Simple Neural Network to Teach Others.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidAcode/Edgar/blob/master/A_Simple_Neural_Network_to_Teach_Others.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "nk_WM9N0-Dx9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#I've broken this down into 13 numbered steps:\n",
        "#1 Sigmoid Function (has 2 parts): \n",
        "def nonlin(x,deriv=False):\n",
        "# When called, Line 12 determines the slope (aka, the \"gradient\") of the \n",
        "#S-shaped curve that is created by line 13 below.  A number near 1 suggests \n",
        "#high-confidence that the network's prediction was correct.  Near 0 suggests \n",
        "#high confidence that the prediction was wrong.  All numbers in the middle \n",
        "#suggest uncertainty, and the system focuses on tweaking those in the right \n",
        "#direction towards a global minimum cost error, i.e. a confident, accurate, network.\n",
        "  if(deriv==True):\n",
        "    return x*(1-x)\n",
        "\n",
        "#Line 17 transforms any value in an array into a number between 0 and 1--a \n",
        "#\"probability\" which then is used to quantify the network's confidence in the \n",
        "#\"prediction\" it just made.  \n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "#2 create input data: X array is 4 training examples and layers of nodes\n",
        "X = np.array([[0,0,1],\n",
        "              [0,1,1],\n",
        "              [1,0,1],\n",
        "              [1,1,1]])\n",
        "\n",
        "#3 create output data: hypothesis of target values\n",
        "y = np.array([[0],\n",
        "              [1],\n",
        "              [1],\n",
        "              [0]])\n",
        "\n",
        "#4 seed your random generation of synapes values to ease debugging\n",
        "np.random.seed(1)\n",
        "\n",
        "#5 randomly initialize synapses with mean 0\n",
        "syn0 = 2*np.random.random((3,4)) - 1 \n",
        "syn1 = 2*np.random.random((4,1)) - 1\n",
        "\n",
        "#6 For loop to iterate through 60,000 tries for the network to \n",
        "#\"guess, tweak the bad predictions, and guess again\"\n",
        "for j in range(60000):\n",
        "\n",
        "  #7 Feed fwd layers 0, 1, 2\n",
        "  l0 = X #this is the array X from line 20--the input layer\n",
        "  #Multiplying each layer by its synapse\n",
        "  #This is our prediction step. Basically, we first let the network \"try\" to \n",
        "  #predict the output given the input. We will then study how it performs so that \n",
        "  #we can adjust it to do a bit better for each iteration. \n",
        "  l1 = nonlin(np.dot(l0,syn0)) #l0 is a 4x3 matrix, so syn0 must be a 3x4 (line 35).\n",
        "  #(4 x 3) dot (3 x 4) = layer1, a (4 x 4) \n",
        "  l2 = nonlin(np.dot(l1,syn1))\n",
        "  #l1(a 4x4) dot syn1(a 4x1) = layer2(a 4x1)\n",
        "  \n",
        "  #Since we loaded in 4 training examples, we ended up with 4 guesses for the \n",
        "  #correct answer, a (4 x 1) matrix (i.e., Layer 2). Each output corresponds with the \n",
        "  #network's guess for a given input.\n",
        "  #8 How much did l2 miss target values (y) by?\n",
        "  l2_error = y - l2\n",
        "  #So, given that l2 had a \"guess\" for each input. We can now compare how \n",
        "  #well it did by subtracting the guess (l2) from the true answer, (y).\n",
        "  #l2_error is just a vector of pos and neg numbers reflecting how much the network missed.\n",
        "  \n",
        "  #9 Print error\n",
        "  if(j% 10000) == 0: #this prints out the error of every 10,000th iteration\n",
        "    print(\"Error: \" + str(np.mean(np.abs(l2_error))))\n",
        "    \n",
        "  #10 In which direction is the target? Our target value y is also our global \n",
        "  #minimum in the gradient descent process of Step 13\n",
        "  # multiply how much we l2's guesses missed their y target by the\n",
        "  # slope of the sigmoid S-curve at the values in l2's 4x1 vector\n",
        "  l2_delta = l2_error*nonlin(l2,deriv=True)\n",
        "  \n",
        "  #11 Back Propogation: How much did l1 contribute to l1 error (according to weights?)\n",
        "  l1_error = l2_delta.dot(syn1.T) #working backwards to find errors in the hidden layer\n",
        "  \n",
        "  #12 In which direction is the target?\n",
        "  # multiply the amount by which l1 missed by the target values by the \n",
        "  # slope of the sigmoid at the values in l1 (slope of the S curve = confidence level)\n",
        "  l1_delta = l1_error*nonlin(l1,deriv=True)\n",
        "  \n",
        "  #13 Gradient Descent: Update synapses so that next guess brings network closer\n",
        "  #to the global minimum. NB: syn1 and syn0 are the true heart of a neural network\n",
        "  #They are where the \"learning\" takes place and is stored. All other values are \n",
        "  #transient.  X may \"seem\" like the heart, but the syn is the real brain.\n",
        "  syn1 += l1.T.dot(l2_delta)\n",
        "  syn0 += l0.T.dot(l1_delta)\n",
        "  \n",
        "print(\"Output After Training:\")\n",
        "print(l2)\n",
        "  \n",
        "              "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xIJfkXIj93zH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}